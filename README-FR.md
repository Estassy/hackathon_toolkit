# Hackathon : Apprentissage par renforcement pour la navigation de drones

_Votre t√¢che est de d√©velopper un algorithme d'apprentissage par renforcement (RL) pour enseigner aux drones √† naviguer de mani√®re autonome dans un environnement en grille, atteignant leur objectif le plus rapidement possible tout en √©vitant les obstacles. L'environnement inclut des murs et des obstacles dynamiques que les drones doivent contourner pour r√©ussir leur mission._

<img src="img/env.png" alt="Exemple d'environnement" style="width:300px; display: block; margin: 0 auto"/>

## üåç Pr√©sentation du projet

Cet ensemble d'outils fournit un environnement simul√© dans lequel vos drones doivent naviguer. L'objectif est que vos drones prennent des actions optimales pour atteindre leur cible rapidement tout en √©vitant les collisions avec des obstacles dynamiques. L'environnement comprend des murs et des obstacles mobiles qui peuvent changer de position √† chaque √©tape.

Ce guide vous accompagnera dans les premi√®res √©tapes pour utiliser les outils fournis afin de construire et d'entra√Æner votre agent. L'ensemble d'outils inclut les fichiers suivants :

- `requirements.txt` - liste des biblioth√®ques et d√©pendances n√©cessaires pour ex√©cuter le projet¬†;
- `teamname_hackathon.ipynb` - notebook pour entra√Æner et tester votre agent¬†;
- `simulate.py` - ex√©cute des √©pisodes de simulation pour l'entra√Ænement ou le test¬†;
- `env.py` - contient la configuration et la mise en place de l'environnement¬†;
- `agent.py` - contient la logique de la politique et de la s√©lection d'actions de l'agent¬†;
- `reward.py` - contient la fonction de calcul des r√©compenses¬†;
- `config.json` - fichier de configuration pour modifier les param√®tres de l'environnement et des drones.

## üìö Pr√©requis

Avant de commencer, assurez-vous d'avoir install√© les d√©pendances n√©cessaires en ex√©cutant :

```bash
pip install -r requirements.txt
```

## ‚ö° Guide rapide

Le fichier `teamname_hackathon.ipynb` est divis√© en deux sections principales :

- _1. Entra√Ænement_ : la premi√®re cellule vous permettra d'ex√©cuter rapidement des simulations d'entra√Ænement dans l'environnement en utilisant les param√®tres d√©finis dans `config.json`. Dans l'impl√©mentation actuelle, **les actions des agents sont s√©lectionn√©es de mani√®re al√©atoire**, ce qui vous donne un mod√®le de r√©f√©rence (aucun apprentissage n'est appliqu√©).
- _2. √âvaluation_ : cette section vous permet de tester un

agent entra√Æn√© avec des configurations fixes et d'enregistrer les r√©sultats dans un fichier `csv`.

Vous pouvez interrompre la simulation √† tout moment en utilisant `Ctrl+C`.

## ü§ñ Agents

Les drones repr√©sentent les agents dans l'environnement. √Ä chaque √©tape, chaque agent doit choisir l'une des actions suivantes :

- rester immobile ;
- avancer ;
- reculer ;
- se d√©placer √† gauche ;
- se d√©placer √† droite ;
- tourner √† droite ;
- tourner √† gauche.

Chaque agent est √©quip√© d'un syst√®me LIDAR (faisceaux translucides) qui analyse son environnement dans un rayon d√©fini :

- le LIDAR principal scanne devant l'agent avec une port√©e plus longue d√©finie par `max_lidar_dist_main` dans `config.json`¬†;
- les LIDAR secondaires scannent √† gauche et √† droite avec une port√©e plus courte d√©finie par `max_lidar_dist_second` dans `config.json`.

<img src="img/agents.png" alt="Cellules cibles" style="width:70px; display: block; margin: 0 auto"/>

L'agent re√ßoit l'√©tat suivant √† chaque √©tape :

- sa position et orientation actuelles (x, y, o) ;
- son statut (0 : actif, 1 : √©vacu√©, 2 : d√©sactiv√©) ;
- la position de son objectif (x, y) ;
- les donn√©es LIDAR dans les 3 directions (principal, droite, gauche)¬†:
  - la distance jusqu'√† l'obstacle le plus proche (ou la port√©e maximale si aucun obstacle n'est pr√©sent)¬†;
  - le type d'obstacle d√©tect√© (0 : aucun, 1 : mur ou bordure de la grille, 2 : obstacle dynamique, 3 : autre agent).
- pour chaque agent dans la port√©e de communication :
  - leur position et orientation ;
  - leur statut ;
  - leurs donn√©es LIDAR.

Ces informations peuvent √™tre utilis√©es telles quelles pour la s√©lection d'actions, mais vous pouvez √©galement les transformer ou les combiner, √† condition de ne pas ajouter d'informations externes.

Votre objectif est de construire votre propre agent en modifiant la classe `MyAgent()` dans le fichier `agent.py`. Nous vous recommandons d'impl√©menter les m√©thodes n√©cessaires pour s√©lectionner une action, comme `get_action()`, et pour mettre √† jour la politique avec `update_policy()`.

Pour des it√©rations plus rapides ou plus lentes, vous pouvez ajuster la fonction `time.sleep()` entre chaque it√©ration dans `simulate.py`. Pour entra√Æner sans affichage, ce qui peut ralentir les it√©rations, utilisez `render_mode=None` dans `config.json`.

## üèôÔ∏è Environnement

L'environnement est d√©fini dans `env.py` et est g√©n√©r√© semi-al√©atoirement √† l'aide d'une graine. Il n'y a pas de restrictions sur les environnements utilis√©s pendant la phase d'entra√Ænement.

### **Position de d√©part et objectif**

Les drones commencent dans l'un des quatre coins de la grille. L'objectif est toujours situ√© dans le coin oppos√©, marqu√© par une zone verte. Les drones doivent naviguer vers cet objectif.

<img src="img/goal.png" alt="Cellules cibles" style="width:30px; display: block; margin: 0 auto"/>

### **Obstacles**

**Les murs** sont g√©n√©r√©s al√©atoirement sous forme de carr√©s noirs. Les collisions d√©sactivent les drones.

**Les obstacles dynamiques** sont g√©n√©r√©s al√©atoirement sous forme de triangles violets, toujours situ√©s en dehors des zones de d√©part et d'objectif. Ils peuvent se d√©placer ou rester immobiles √† chaque √©tape. Si un agent se rapproche d'une cellule adjacente √† un obstacle dynamique, il est d√©sactiv√©.

<img src="img/dyn_obs.png" alt="Obstacles dynamiques et zones dangereuses" style="width:50px; display: block; margin: 0 auto"/>

## ‚öôÔ∏è Configuration

Pour entra√Æner vos agents et optimiser leur apprentissage, vous pouvez modifier les param√®tres de configuration dans le fichier `config.json`. Les param√®tres disponibles incluent :

- `grid_size` : taille de la grille ;
- `walls_proportion` : proportion de murs dans la grille ;
- `num_dynamic_obstacles` : nombre d'obstacles dynamiques ;
- `num_agents` : nombre de drones ;
- `communication_range` : port√©e maximale de communication entre drones ;
- `max_lidar_dist_main` : port√©e maximale du LIDAR principal ;
- `max_lidar_dist_second` : port√©e maximale des LIDAR secondaires ;
- `max_episodes` : nombre maximal d'√©pisodes avant r√©initialisation ;
- `max_episodes_steps` : nombre maximal d'√©tapes par √©pisode ;
- `render_mode` (optionnel) : mode d'affichage pour Pygame (`"human"`) ; si `None`, aucun affichage ;
- `seed` (optionnel) : graine pour un environnement fixe.

Consultez la section _√âvaluation_ pour plus de d√©tails sur les configurations d'√©valuation.

## üéÅ R√©compense

La fonction de r√©compense, situ√©e dans `reward.py`, fournit un retour aux agents en fonction de leurs actions. L'impl√©mentation actuelle est basique mais peut √™tre modifi√©e pour am√©liorer l'apprentissage. Voici comment elle fonctionne :

- un agent d√©sactiv√© (collision) re√ßoit une p√©nalit√© de -100¬†;
- un agent atteignant l'objectif re√ßoit une r√©compense de +1000¬†;
- pour tous les autres agents, une p√©nalit√© de -0,1 par √©tape est appliqu√©e.

Vous √™tes encourag√©s √† modifier cette fonction pour cr√©er un syst√®me de r√©compense plus optimal afin d'am√©liorer les performances d'apprentissage de vos agents.

## ‚úíÔ∏è √âvaluation

Le 7·µâ jour du hackathon √† 00h01, nous fournirons un dossier contenant 10 configurations d'√©valuation, telles que `"eval_configs/config_1.json"`. Vous devrez ex√©cuter la section _2. √âvaluation_ du notebook `teamname_hackathon.ipynb` pour tester les performances de vos agents sur ces configurations.

Renommez le notebook avec le nom de votre √©quipe et mettez √† jour la premi√®re cellule avec les noms des membres de votre √©quipe.

### Livrables

Avant la fin du 7·µâ jour (23h59), soumettez les fichiers suivants dans une archive `.zip` √† contact.hackathon-ai.ah@airbus.com :

- `teamname_hackathon.ipynb` ex√©cut√© avec les sorties et les points de contr√¥le ;
- `agent.py` avec les fonctions d'action et de politique mises √† jour¬†;
- `reward.py` avec la fonction de r√©compense modifi√©e¬†;
- `simulate.py` si des modifications ont √©t√© apport√©es¬†;
- `reward_curve_per_episode.png` g√©n√©r√© automatiquement¬†;
- `all_results.csv` g√©n√©r√© par *√âvaluation*¬†;
- `averages.csv`¬†;
- `README.md` documentant votre code, vos modifications et votre processus d'entra√Ænement¬†;
- Une vid√©o de 2 minutes pr√©sentant votre √©quipe et montrant votre solution √† un public non expert.

Si la taille des fichiers est trop grande, utilisez un lien WeTransfer.

### Crit√®res d'√©valuation

Vos performances seront √©valu√©es comme suit :

- **75¬†%** Performance sur 10 environnements de test¬†;
- **5¬†%** Fonction de r√©compense¬†;
- **20¬†%** Courbe d'apprentissage et strat√©gie.

## üèÅ Bonne chance !

Soyez cr√©atifs, exp√©rimentez diff√©rentes approches, et amusez-vous¬†!

N'h√©sitez pas √† nous contacter en cas de questions √† contact.hackathon-ai.ah@airbus.com

Bon codage¬†! ü§ñ

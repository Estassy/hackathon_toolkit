import numpy as np

def compute_reward(num_agents, old_positions, agent_positions, 
                   evacuated_agents, deactivated_agents, goal_area):
    rewards = np.zeros(num_agents)

    for i, (old_pos, new_pos) in enumerate(zip(old_positions, agent_positions)):
        if i in evacuated_agents:
            continue  # Pas de pÃ©nalitÃ© si dÃ©jÃ  Ã©vacuÃ©

        elif i in deactivated_agents:
            rewards[i] = -50.0  # Moins sÃ©vÃ¨re que -100 pour Ã©viter un blocage complet

        elif tuple(new_pos) in goal_area:
            rewards[i] = 1000.0  # Objectif atteint ğŸ¯
            evacuated_agents.add(i)

        else:
            # Distance avant/aprÃ¨s
            old_dist = np.linalg.norm(np.array(old_pos) - np.array(goal_area[i]))
            new_dist = np.linalg.norm(np.array(new_pos) - np.array(goal_area[i]))

            # âœ… **RÃ©compense un meilleur dÃ©placement**
            if new_dist < old_dist:
                rewards[i] += 5.0  # Encouragement + fort pour avancer  
            else:
                rewards[i] -= 1.0  # PÃ©nalitÃ© plus faible pour Ã©viter un effet "blocage"

            # ğŸ”„ **Bonus pour un agent qui Ã©vite un mur**
            if new_dist == old_dist:  # Cas oÃ¹ l'agent nâ€™a pas pu avancer
                rewards[i] -= 0.2  # LÃ©gÃ¨re punition pour Ã©viter l'immobilitÃ©

            # ğŸ”´ **RÃ©duction de la pÃ©nalitÃ© par step**
            rewards[i] -= 0.05  # Juste pour Ã©viter qu'il reste trop longtemps

    return rewards, evacuated_agents
